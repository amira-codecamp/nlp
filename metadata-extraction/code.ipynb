{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Required packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWpYjIAcv7U0"
      },
      "outputs": [],
      "source": [
        "!pip install -I pillow==7.2.0\n",
        "!sudo apt-get update\n",
        "!pip install pdf2image\n",
        "!apt-get install poppler-utils \n",
        "!sudo apt install tesseract-ocr\n",
        "!sudo apt-get install tesseract-ocr-all\n",
        "!pip install cv2\n",
        "!pip install pytesseract\n",
        "!pip install arabic-reshaper\n",
        "!pip install PyArabic\n",
        "!pip install pypdfium2\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "import shutil\n",
        "import os\n",
        "import os.path\n",
        "from os import path\n",
        "import random\n",
        "from PIL import Image\n",
        "import cv2 \n",
        "import arabic_reshaper\n",
        "from pyarabic.araby import strip_tatweel, strip_tashkeel, normalize_hamza\n",
        "import re\n",
        "import pypdfium2 as pdfium\n",
        "from itertools import chain"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "qWn8JY_Cv4g6"
      },
      "outputs": [],
      "source": [
        "# get text from pdf document\n",
        "def get_text(f_path, language, ocr):\n",
        "\n",
        "    text_all = '' \n",
        "\n",
        "    if ocr == True:\n",
        "        # convert pdf -> images\n",
        "        images = convert_from_path(f_path) \n",
        "\n",
        "        # loop over images\n",
        "        i = 1\n",
        "        for img in images:\n",
        "\n",
        "            # limit to first and last 20 pages & ignore first page\n",
        "            if (i >= 2 & i <= 20) | (i >= len(images) - 20  & i <= len(images)):\n",
        "\n",
        "                if os.path.exists(re.sub('.pdf', '/', f_path)) == False: # create directory\n",
        "                    os.mkdir(re.sub('.pdf', '/', f_path))\n",
        "                img_path = re.sub('.pdf', '/', f_path) + str(i) + '.jpg'\n",
        "                img.save(img_path, 'JPEG') # save image\n",
        "                    \n",
        "                text = pytesseract.image_to_string(cv2.imread(img_path), lang = language) # ocr\n",
        "\n",
        "                text_all = text_all + '\\n' + text # store text\n",
        "            i = i + 1\n",
        "\n",
        "    else:\n",
        "          # open pdf document\n",
        "          pdf = pdfium.PdfDocument(f_path) \n",
        "\n",
        "          # limit to first and last 20 pages & ignore first page\n",
        "          if len(pdf) <= 20:\n",
        "              n_pages = range(1, len(pdf))\n",
        "          else:\n",
        "              n_pages = chain(range(1, 20), range(len(pdf)-20, len(pdf)))\n",
        "\n",
        "          # loop over pages\n",
        "          for i in n_pages:\n",
        "\n",
        "              page = pdf[i] # get page\n",
        "\n",
        "              textpage = page.get_textpage()\n",
        "              text = textpage.get_text_range() # extract text\n",
        "\n",
        "              text = re.sub('\\r\\n', ' ', text) # clean text\n",
        "\n",
        "              text_all = text_all + '\\n' + text # store text\n",
        "\n",
        "    return text_all\n",
        "\n",
        "# clean text from noise\n",
        "def clean(text):\n",
        "\n",
        "    new_text = ''\n",
        "\n",
        "    # define patterns\n",
        "    regex = 'dédicas|dedicas|notation et symboles|remerciements|remerciement|acknowledgement|dédicaces|dedicaces|dédicace|dedicace|liste des figures|liste-des-figures|list of figures|list-of-figures|liste des abreviations|liste-des-abreviations|liste des abréviations|liste-des-abréviations|list of abbreviations|list-of-abbreviations|table des matières|table des matieres|table-des-matières|table-des-matieres|sommaire|table of contents|table-of-contents|liste des tableaux|liste-des-tableaux|list of tables|list-of-tables|annexe|références|references|bibliographie|références bibliographiques|références-bibliographiques|bibliographic references|bibliographic-references'\n",
        "    ar_regex = arabic_reshaper.reshape('المراجع|اهدﺍﺀ|ﺷﻜﺮ ﻭﺗﻘﺪﯾﺮ|قائمة-الاشكال|قائمة الاشكال|تقدير|قائمة-الاختصارات|قائمة الاختصارات|قائمة جدول|قائمة جداول|قائمة-جداول|قائمة-جدول|فهرس|الفهرس|جدول المحتويات|قائمة المحتويات|قائمة-المحتويات|جدول-المحتويات|ﺷﻜﺮ ﻭ ﻋﺮﻓﺎﻥ|ﺷﻜﺮ ﻭﻋﺮﻓﺎﻥ|ﺷﻜﺮ ﻭ ﺗﻘﺪﯾﺮ|فهرس ﺍﻟﻤﻮﺍﺿﻴﻊ')\n",
        "     \n",
        "    # loop over pages\n",
        "    text = text.split('\\n')\n",
        "    for p in text: \n",
        "\n",
        "        if re.search(regex, p, re.IGNORECASE) or re.search(ar_regex, p, re.IGNORECASE): # remove outlier pages\n",
        "            pass\n",
        "\n",
        "        else:\n",
        "\n",
        "            p = arabic_reshaper.reshape(p) # reshape arabic\n",
        "            # remove diactrics\n",
        "            p = strip_tatweel(p)\n",
        "            p = strip_tashkeel(p)\n",
        "\n",
        "            new_text = new_text + '\\n' + p # store text\n",
        "   \n",
        "    return new_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "tZAO2jTLHsoX"
      },
      "outputs": [],
      "source": [
        "# find abstract by regex\n",
        "def get_abstract(text):\n",
        "    \n",
        "    en_abstract = ''\n",
        "    fr_abstract = ''\n",
        "    ar_abstract = ''\n",
        "\n",
        "    # define patterns\n",
        "    en_regex = 'abstract|summary'\n",
        "    fr_regex = 'résumé|resume|résume|resumé'\n",
        "    ar_regex = arabic_reshaper.reshape('ملخص|الملخص')\n",
        "\n",
        "    # loop over pages\n",
        "    pages = text.split('\\n')\n",
        "    for p in pages:\n",
        "\n",
        "        last_pos = len(p) # get last index\n",
        "        mtch = re.search(en_regex, p, re.IGNORECASE)\n",
        "        if mtch and en_abstract == '': # extract english abstract\n",
        "            position = mtch.start()\n",
        "            en_abstract = p[position : min(last_pos, position + 3000)] \n",
        "            if len(en_abstract) <= 50: en_abstract = '' \n",
        "\n",
        "        mtch = re.search(fr_regex, p, re.IGNORECASE)\n",
        "        if mtch and fr_abstract == '': # extract french abstract\n",
        "            position = mtch.start()\n",
        "            fr_abstract = p[position : min(last_pos, position + 3000)] \n",
        "            if len(fr_abstract) <= 50: fr_abstract = '' \n",
        "            \n",
        "        mtch = re.search(ar_regex, p, re.IGNORECASE)\n",
        "        if mtch and ar_abstract == '': # extract arabic abstract\n",
        "            position = mtch.start()\n",
        "            ar_abstract = p[position : min(last_pos, position + 3000)]     \n",
        "            if len(ar_abstract) <= 50: ar_abstract = '' \n",
        "\n",
        "    return [en_abstract, fr_abstract, ar_abstract]\n",
        "\n",
        "# find keywords by regex\n",
        "def get_keywords(text):\n",
        "\n",
        "    en_keywords = ''\n",
        "    fr_keywords = ''\n",
        "    ar_keywords = ''\n",
        "    \n",
        "    # define patterns\n",
        "    en_regex = 'keywords|key words|key-words|index terms|index-terms'\n",
        "    fr_regex = 'mots clés|mots clé|mots-clés|mots-clé|mots cles|mots cle|mots-cles|mots-cle|termes indexation|termes-indexation'\n",
        "    ar_regex = arabic_reshaper.reshape('الكلمات المفتاحية|الكلمات-المفتاحية|الكلمات الافتتاحية|الكلمات-الافتتاحية|الكلمات المفتاح|الكلمات-المفتاح|كلمات مفتاحية|كلمات-مفتاحية|كلمات افتتاحية|كلمات-افتتاحية|كلمات مفتاح|كلمات-مفتاح')\n",
        "\n",
        "    # loop over pages\n",
        "    pages = text.split('\\n')\n",
        "    for p in pages:\n",
        "\n",
        "        last_pos = len(p) # get last index\n",
        "        mtch = re.search(en_regex, p, re.IGNORECASE)\n",
        "        if mtch and en_keywords == '': # extract english keywords\n",
        "            position = mtch.start()\n",
        "            en_keywords = p[position : min(last_pos, position + 250)] \n",
        "\n",
        "        mtch = re.search(fr_regex, p, re.IGNORECASE)\n",
        "        if mtch and fr_keywords == '': # extract french keywords\n",
        "            position = mtch.start()\n",
        "            fr_keywords = p[position : min(last_pos, position + 250)] \n",
        "            \n",
        "        mtch = re.search(ar_regex, p, re.IGNORECASE)\n",
        "        if mtch and ar_keywords == '': # extract arabic keywords\n",
        "            position = mtch.start()\n",
        "            ar_keywords = p[position : min(last_pos, position + 250)]  \n",
        "\n",
        "    return [en_keywords, fr_keywords, ar_keywords]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PIkUWyVRuNEE"
      },
      "source": [
        "****"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Main**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "7XZhw78OcF6k"
      },
      "outputs": [],
      "source": [
        "def main(f_path, lang = 'fra', ocr = False): \n",
        "  \n",
        "    text = clean(get_text(f_path, lang, ocr))  # by default extract without ocr\n",
        "    ena, fra, ara = get_abstract(text)\n",
        "    enk, frk, ark = get_keywords(text)\n",
        "\n",
        "    regex = 'keywords|key words|key-words|index terms|index-terms|mots cles|mots cle|mots-cles|mots-cle|mots clés|mots clé|mots-clés|mots-clé|termes indexation|termes-indexation'\n",
        "    ar_regex = arabic_reshaper.reshape('الكلمات المفتاحية|الكلمات-المفتاحية|الكلمات الافتتاحية|الكلمات-الافتتاحية|الكلمات المفتاح|الكلمات-المفتاح|كلمات مفتاحية|كلمات-مفتاحية|كلمات افتتاحية|كلمات-افتتاحية|كلمات مفتاح|كلمات-مفتاح')\n",
        "    if re.search(regex, ena, re.IGNORECASE): ena = ena[0: re.search(regex, ena, re.IGNORECASE).start()]\n",
        "    if re.search(ar_regex, ena, re.IGNORECASE): ena = ena[0: re.search(ar_regex, ena, re.IGNORECASE).start()]\n",
        "    if re.search(regex, fra, re.IGNORECASE): fra = fra[0: re.search(regex, fra, re.IGNORECASE).start()]\n",
        "    if re.search(ar_regex, fra, re.IGNORECASE): fra = fra[0: re.search(ar_regex, fra, re.IGNORECASE).start()]\n",
        "    if re.search(regex, ara, re.IGNORECASE): ara = ara[0: re.search(regex, ara, re.IGNORECASE).start()]\n",
        "    if re.search(ar_regex, ara, re.IGNORECASE): ara = ara[0: re.search(ar_regex, ara, re.IGNORECASE).start()]\n",
        "    if re.search(regex, enk, re.IGNORECASE): enk = enk[re.search(regex, enk, re.IGNORECASE).end() ::]\n",
        "    if re.search(regex, frk, re.IGNORECASE): frk = frk[re.search(regex, frk, re.IGNORECASE).end() ::]\n",
        "    if re.search(ar_regex, ark, re.IGNORECASE): ark = ark[re.search(ar_regex, ark, re.IGNORECASE).end() ::]\n",
        "\n",
        "    regex = 'abstract|summary|résumé|resumé|résume|resume'\n",
        "    ar_regex = arabic_reshaper.reshape('ملخص|الملخص')\n",
        "    if re.search(regex, enk, re.IGNORECASE): enk = enk[0: re.search(regex, enk, re.IGNORECASE).start()]\n",
        "    if re.search(ar_regex, enk, re.IGNORECASE): enk = enk[0: re.search(ar_regex, enk, re.IGNORECASE).start()]\n",
        "    if re.search(regex, frk, re.IGNORECASE): frk = frk[0: re.search(regex, frk, re.IGNORECASE).start()]\n",
        "    if re.search(ar_regex, frk, re.IGNORECASE): frk = frk[0: re.search(ar_regex, frk, re.IGNORECASE).start()]\n",
        "    if re.search(regex, ark, re.IGNORECASE): ark = ark[0: re.search(regex, ark, re.IGNORECASE).start()]\n",
        "    if re.search(ar_regex, ark, re.IGNORECASE): ark = ark[0: re.search(ar_regex, ark, re.IGNORECASE).start()]\n",
        "    if re.search(regex, ena, re.IGNORECASE): ena = ena[re.search(regex, ena, re.IGNORECASE).end() ::]\n",
        "    if re.search(regex, fra, re.IGNORECASE): fra = fra[re.search(regex, fra, re.IGNORECASE).end() ::]\n",
        "    if re.search(ar_regex, ara, re.IGNORECASE): ara = ara[re.search(ar_regex, ara, re.IGNORECASE).end() ::]\n",
        "    \n",
        "    ena = ena.lstrip()\n",
        "    ena = ena.lstrip(':')\n",
        "    ena = ena.lstrip()\n",
        "    fra = fra.lstrip()\n",
        "    fra = fra.lstrip(':')\n",
        "    fra = fra.lstrip()\n",
        "    ara = ara.lstrip()\n",
        "    ara = ara.lstrip(':')\n",
        "    ara = ara.lstrip()\n",
        "    enk = enk.lstrip()\n",
        "    enk = enk.lstrip(':')\n",
        "    enk = enk.lstrip()\n",
        "    frk = frk.lstrip()\n",
        "    frk = frk.lstrip(':')\n",
        "    frk = frk.lstrip()\n",
        "    ark = ark.lstrip()\n",
        "    ark = ark.lstrip(':')\n",
        "    ark = ark.lstrip()\n",
        "\n",
        "    abstract = ena + '\\n' + fra + '\\n' + ara\n",
        "    keywords = enk + '\\n' + frk + '\\n' + ark\n",
        "\n",
        "    return [abstract, keywords, text]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
