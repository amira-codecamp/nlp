{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6fYVBpGHqEd8"
      },
      "source": [
        "**Keyword extraction by Yake - From Abstract**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERtx6GLVMX1W"
      },
      "outputs": [],
      "source": [
        "!pip install yake\n",
        "!pip install langdetect\n",
        "!pip install fuzzywuzzy\n",
        "import yake\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from langdetect import detect\n",
        "from fuzzywuzzy import process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_keywords(text):\n",
        "    \n",
        "    # remove stop-words\n",
        "    stopwords = set(nltk.corpus.stopwords.words('english')) | set(nltk.corpus.stopwords.words('french')) | set(nltk.corpus.stopwords.words('arabic'))\n",
        "    text = ' '.join([word for word in word_tokenize(text) if word.lower() not in stopwords])\n",
        "\n",
        "    # keep only nouns, adverbs and adjectives\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged = nltk.pos_tag(tokens)\n",
        "    tags = ['NN', 'NNP', 'NNS', 'NNPS', 'JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS']\n",
        "    text = ' '.join([t[0] for t in tagged if t[1] in tags])\n",
        "    \n",
        "    # yake extraction\n",
        "    lang = detect(text)\n",
        "    kw_extractor = yake.KeywordExtractor(lan=lang, n=2, top=100) # limit to best 100 uni/bi grams \n",
        "    results = kw_extractor.extract_keywords(text)\n",
        "    keywords = [word for word, score in results]\n",
        "    \n",
        "    # remove duplicates\n",
        "    keywords = process.dedupe(keywords, threshold=25) # similarity percentage 25%\n",
        "\n",
        "    # format to string\n",
        "    keywords = ', '.join(keywords)\n",
        "\n",
        "    return keywords"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Abstract / Keyword extraction by Cermine - Articles**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1LzstOipts8"
      },
      "outputs": [],
      "source": [
        "# install java\n",
        "!apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "!java -version\n",
        "# install cermine\n",
        "!wget https://maven.ceon.pl/artifactory/kdd-releases/pl/edu/icm/cermine/cermine-impl/1.13/cermine-impl-1.13-jar-with-dependencies.jar\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "from bs4 import BeautifulSoup\n",
        "from functools import reduce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t91GvcHPp6Be"
      },
      "outputs": [],
      "source": [
        "# get abstract from cermine xml output\n",
        "def get_file_abstract(f):\n",
        "  try :\n",
        "    with open (f,'r',encoding='utf-8') as file:\n",
        "        content = file.readlines()\n",
        "        content = \"\".join(content)\n",
        "        soup = BeautifulSoup(content)\n",
        "        abstracts = soup.findAll(\"abstract\")\n",
        "        abstracts_array =  [abstract.text for abstract in abstracts]\n",
        "        return(abstracts_array[0])\n",
        "  except:\n",
        "      return \"\"\n",
        "\n",
        "# get keywords from cermine xml output\n",
        "def get_file_keywords(f):\n",
        "  try :\n",
        "    with open (f,'r',encoding='utf-8') as file:\n",
        "        content = file.readlines()\n",
        "        content = \"\".join(content)\n",
        "        soup = BeautifulSoup(content)\n",
        "        keys = soup.findAll(\"keywords\")\n",
        "        keys_array =  [key.text for key in keys]\n",
        "        return(keys_array[0])\n",
        "  except:\n",
        "      return \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cT4jHzPeqDOh"
      },
      "outputs": [],
      "source": [
        "!java -cp cermine-impl-1.13-jar-with-dependencies.jar pl.edu.icm.cermine.ContentExtractor -path 'directory_path'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yirJ6xWqW1t"
      },
      "outputs": [],
      "source": [
        "def extract_cermine(directory_path, file_name_without_extension):\n",
        "    return [get_file_abstract(directory_path + file_name_without_extension + '.cermxml'), get_file_keywords(directory_path + file_name_without_extension + '.cermxml')]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
