{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5b3Eu5XlkA_"
      },
      "outputs": [],
      "source": [
        "# Install and Import Python Librairies \n",
        "!pip install alphabet-detector\n",
        "!pip install pyarabic\n",
        "!pip install arabic-reshaper\n",
        "!pip install nltk\n",
        "!pip install rapidfuzz\n",
        "!pip install jarowinkler\n",
        "!pip install jellyfish\n",
        "!pip install xlsxwriter\n",
        "import pandas\n",
        "import numpy\n",
        "import re\n",
        "import hashlib\n",
        "from alphabet_detector import AlphabetDetector\n",
        "import pyarabic.araby as araby\n",
        "import arabic_reshaper\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "nltk.download('punkt')\n",
        "from rapidfuzz import process, fuzz\n",
        "from rapidfuzz.process import cdist\n",
        "from jarowinkler import jarowinkler_similarity\n",
        "import jellyfish\n",
        "import networkx as nx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MT0yJkBDllv5"
      },
      "outputs": [],
      "source": [
        "# Normalization Libraries\n",
        "\n",
        "def normalize_hamza(text):\n",
        "    # Alif-Hamza\n",
        "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
        "    # Waw-Hamza\n",
        "    text = re.sub(\"ؤ\", \"ء\", text)\n",
        "    # Yay-Hamza\n",
        "    text = re.sub(\"ئ\", \"ء\", text)\n",
        "    return text\n",
        "\n",
        "def remove_accents(raw_text):\n",
        "    raw_text = re.sub(u\"[àáâãäå]\", 'a', raw_text)\n",
        "    raw_text = re.sub(u\"[èéêë]\", 'e', raw_text)\n",
        "    raw_text = re.sub(u\"[ìíîï]\", 'i', raw_text)\n",
        "    raw_text = re.sub(u\"[òóôõö]\", 'o', raw_text)\n",
        "    raw_text = re.sub(u\"[ùúûü]\", 'u', raw_text)\n",
        "    raw_text = re.sub(u\"[ýÿ]\", 'y', raw_text)\n",
        "    raw_text = re.sub(u\"ß\", 'ss', raw_text)\n",
        "    raw_text = re.sub(u\"ñ\", 'n', raw_text)\n",
        "    raw_text = re.sub(u\"ç\", 'c', raw_text)\n",
        "    return raw_text\n",
        "\n",
        "def normalize(list_n):\n",
        "    # Convert to String\n",
        "    string = numpy.vectorize(str)\n",
        "    list_n = string(list_n)\n",
        "    # Lower case\n",
        "    lower = numpy.vectorize(numpy.char.lower)\n",
        "    list_n = lower(list_n)\n",
        "    # Remove arabic diacritics\n",
        "    diactrics = numpy.vectorize(araby.strip_diacritics)\n",
        "    list_n = diactrics(list_n)\n",
        "    # Remove arabic tatweel\n",
        "    tatweel = numpy.vectorize(araby.strip_tatweel)\n",
        "    list_n = tatweel(list_n)\n",
        "    # Normalize types of hamza\n",
        "    hamza = numpy.vectorize(normalize_hamza)\n",
        "    list_n = hamza(list_n)\n",
        "    # Reshape arabic text\n",
        "    reshaper = numpy.vectorize(arabic_reshaper.reshape)\n",
        "    list_n = reshaper(list_n)\n",
        "    # Remove accents\n",
        "    accents = numpy.vectorize(remove_accents)\n",
        "    list_n = accents(list_n)\n",
        "    # Remove punctuations\n",
        "    punctuation = numpy.vectorize(re.sub)\n",
        "    list_n = punctuation(r'\\W', ' ', list_n)   \n",
        "    # Remove numbers\n",
        "    punctuation = numpy.vectorize(re.sub)\n",
        "    list_n = punctuation(r'\\d+', '', list_n)\n",
        "    # Capitalize \n",
        "    capitalize = numpy.vectorize(str.title)\n",
        "    list_n = capitalize(list_n)\n",
        "    # Remove additional spaces\n",
        "    space = numpy.vectorize(re.sub)\n",
        "    list_n = space(' +', ' ', list_n)\n",
        "    space = numpy.vectorize(str.lstrip)\n",
        "    list_n = space(list_n)\n",
        "    space = numpy.vectorize(str.rstrip)\n",
        "    list_n = space(list_n)\n",
        "    return list_n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OUDAKyIZYtKC"
      },
      "outputs": [],
      "source": [
        "# Tokenization Library\n",
        "\n",
        "def tokenizer(name):\n",
        "    name = TreebankWordDetokenizer().detokenize(sorted(word_tokenize(name))) \n",
        "    return name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6TcTjbwvlo8G"
      },
      "outputs": [],
      "source": [
        "# Transliteration Libraries\n",
        "\n",
        "def load():\n",
        "    # Read dictionary\n",
        "    dictionary = pandas.read_csv(\"dictionary.csv\", usecols=['arabic', 'roman'])\n",
        "    arabic = numpy.array(dictionary['arabic'])\n",
        "    roman = numpy.array(dictionary['roman'])\n",
        "    # Define transliterator\n",
        "    to_latin = {\n",
        "        'ا': 'a', 'ء': 'a', 'إ': 'i', 'ى': 'a', 'بو': 'bou', 'ب': 'b', 'تو': 'tou', 'ت': 't', 'ثو': 'thou', 'ث': 'th',\n",
        "        'جو': 'djou', 'ج': 'dj', 'حو': 'hou', 'حي': 'hy',\n",
        "        'ح': 'h', 'خو': 'khou', 'خ': 'kh', 'دو': 'dou', 'د': 'd', 'ذو': 'dou', 'ذ': 'd', 'رو': 'rou', 'ري': 'ry',\n",
        "        'ر': 'r', 'زو': 'zou', 'ز': 'z', 'سو': 'sou', 'س': 's',\n",
        "        'شو': 'chou', 'ش': 'ch', 'صو': 'sou', 'ص': 's', 'ضو': 'dou', 'ض': 'd', 'طو': 'tou', 'ط': 't', 'ظو': 'dou',\n",
        "        'ظ': 'd', 'غو': 'ghou', 'غ': 'gh', 'فو': 'fou', 'ف': 'f',\n",
        "        'قو': 'qou', 'ق': 'q', 'كو': 'kou', 'ك': 'k', 'لو': 'lou', 'ل': 'l', 'مو': 'mou', 'م': 'm', 'نو': 'nou',\n",
        "        'ن': 'n', 'هو': 'hou', 'ه': 'h', 'وو': 'wou', 'و': 'w',\n",
        "        'يو': 'you', 'يي': 'yi', 'ي': 'i', 'ع': 'a', 'ة': 'a', 'ھ': 'h', 'ی': 'a', ' ':' '\n",
        "    }\n",
        "    return arabic, roman, to_latin\n",
        "\n",
        "def translit_arabic(text, arabic, roman, to_latin):\n",
        "    text = text.split(' ')\n",
        "    for part in text:\n",
        "        # Use dictionary\n",
        "        if part in arabic:\n",
        "            new = roman[arabic == part][0]\n",
        "        # Use transliterator\n",
        "        else:\n",
        "            for k, v in to_latin.items():\n",
        "                new = re.sub(k, v, part)\n",
        "        text =list(map(lambda x: x.replace(part, new), text))\n",
        "    text = ' '.join(text)\n",
        "    return text"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw6xOnizFZlS"
      },
      "source": [
        "***Name Disambiguation***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kC2ppLLj0B_L"
      },
      "outputs": [],
      "source": [
        "def main(authors):\n",
        "    # Load dictionary\n",
        "    arabic, roman, to_latin = load()   \n",
        "    # Normalization\n",
        "    authors = normalize(authors)\n",
        "    # Tokenization\n",
        "    authors_r = [tokenizer(name) for name in authors]\n",
        "    # Transliteration\n",
        "    authors_r = [translit_arabic(name, arabic, roman, to_latin) for name in authors_r]\n",
        "    # Jaro Winkler comparison\n",
        "    distances = cdist(authors_r, authors_r, scorer=jarowinkler_similarity, score_cutoff=0.9, dtype=numpy.uint8, workers=-1)\n",
        "    indx_l = numpy.stack(numpy.where(distances >= 0.9), axis = 1)\n",
        "    # Match Rating comparison\n",
        "    rates = numpy.array([jellyfish.jaro_distance(jellyfish.match_rating_codex(authors[indx[0]]),\n",
        "                            jellyfish.match_rating_codex(authors[indx[1]])) for indx in indx_l])\n",
        "    indx_l = indx_l[numpy.where(rates >= 0.9)]\n",
        "    # Group similar\n",
        "    g = nx.Graph()\n",
        "    for p in indx_l:\n",
        "        g.add_edges_from(zip(p, p[1:]))\n",
        "    g = nx.connected_components(g)\n",
        "    # Standardization\n",
        "    for c in g:\n",
        "        c = list(c)\n",
        "        for i in range(len(c)):\n",
        "            if authors[c[i]] != authors[c[0]]:\n",
        "                print(authors[c[0]])\n",
        "                authors[c[i]] = authors[c[0]]\n",
        "    return authors"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
