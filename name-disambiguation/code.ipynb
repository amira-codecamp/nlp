{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48jpdJsUbHoM"
      },
      "source": [
        "**Python packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5b3Eu5XlkA_"
      },
      "outputs": [],
      "source": [
        "!pip install alphabet-detector\n",
        "!pip install pyarabic\n",
        "!pip install nltk\n",
        "!pip install rapidfuzz\n",
        "!pip install jarowinkler\n",
        "!pip install jellyfish\n",
        "!pip install xlsxwriter\n",
        "import pandas\n",
        "import numpy\n",
        "import re\n",
        "import hashlib\n",
        "from alphabet_detector import AlphabetDetector\n",
        "import pyarabic.araby as araby\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "nltk.download('punkt')\n",
        "from rapidfuzz import process, fuzz\n",
        "from rapidfuzz.process import cdist\n",
        "from jarowinkler import jarowinkler_similarity\n",
        "import jellyfish\n",
        "import networkx as nx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pQv3nMcpwvR"
      },
      "source": [
        "**Normalization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MT0yJkBDllv5"
      },
      "outputs": [],
      "source": [
        "def normalize_hamza(text):\n",
        "    # Alif-Hamza\n",
        "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
        "    # Waw-Hamza\n",
        "    text = re.sub(\"ؤ\", \"ء\", text)\n",
        "    # Yay-Hamza\n",
        "    text = re.sub(\"ئ\", \"ء\", text)\n",
        "    return text\n",
        "\n",
        "def remove_accents(raw_text):\n",
        "    raw_text = re.sub(u\"[àáâãäå]\", 'a', raw_text)\n",
        "    raw_text = re.sub(u\"[èéêë]\", 'e', raw_text)\n",
        "    raw_text = re.sub(u\"[ìíîï]\", 'i', raw_text)\n",
        "    raw_text = re.sub(u\"[òóôõö]\", 'o', raw_text)\n",
        "    raw_text = re.sub(u\"[ùúûü]\", 'u', raw_text)\n",
        "    raw_text = re.sub(u\"[ýÿ]\", 'y', raw_text)\n",
        "    raw_text = re.sub(u\"ß\", 'ss', raw_text)\n",
        "    raw_text = re.sub(u\"ñ\", 'n', raw_text)\n",
        "    raw_text = re.sub(u\"ç\", 'c', raw_text)\n",
        "    return raw_text\n",
        "\n",
        "def normalize(list_n):\n",
        "    # Convert to String\n",
        "    string = numpy.vectorize(str)\n",
        "    list_n = string(list_n)\n",
        "    # Lower case\n",
        "    lower = numpy.vectorize(numpy.char.lower)\n",
        "    list_n = lower(list_n)\n",
        "    # Remove arabic diacritics\n",
        "    diactrics = numpy.vectorize(araby.strip_diacritics)\n",
        "    list_n = diactrics(list_n)\n",
        "    # Remove arabic tatweel\n",
        "    tatweel = numpy.vectorize(araby.strip_tatweel)\n",
        "    list_n = tatweel(list_n)\n",
        "    # Normalize types of hamza\n",
        "    hamza = numpy.vectorize(normalize_hamza)\n",
        "    list_n = hamza(list_n)\n",
        "    # Remove accents\n",
        "    accents = numpy.vectorize(remove_accents)\n",
        "    list_n = accents(list_n)\n",
        "    # Remove punctuations\n",
        "    punctuation = numpy.vectorize(re.sub)\n",
        "    list_n = punctuation(r'\\W', ' ', list_n)   \n",
        "    # Remove numbers\n",
        "    punctuation = numpy.vectorize(re.sub)\n",
        "    list_n = punctuation(r'\\d+', '', list_n)\n",
        "    return list_n\n",
        "\n",
        "def sort(name):\n",
        "    name = TreebankWordDetokenizer().detokenize(sorted(word_tokenize(name))) \n",
        "    return name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjPk0WX_pwvT"
      },
      "source": [
        "**Transliteration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6TcTjbwvlo8G"
      },
      "outputs": [],
      "source": [
        "def load_():\n",
        "    # Read dictionary\n",
        "    dictionary = pandas.read_csv(\"database.csv\", usecols=['arabic', 'roman'])\n",
        "    arabic = numpy.array(dictionary['arabic'])\n",
        "    roman = numpy.array(dictionary['roman'])\n",
        "    # Define transliterator\n",
        "    to_latin = {\n",
        "        'ا': 'a', 'ء': 'a', 'إ': 'i', 'ى': 'a', 'بو': 'bou', 'ب': 'b', 'تو': 'tou', 'ت': 't', 'ثو': 'thou', 'ث': 'th',\n",
        "        'جو': 'djou', 'ج': 'dj', 'حو': 'hou', 'حي': 'hy',\n",
        "        'ح': 'h', 'خو': 'khou', 'خ': 'kh', 'دو': 'dou', 'د': 'd', 'ذو': 'dou', 'ذ': 'd', 'رو': 'rou', 'ري': 'ry',\n",
        "        'ر': 'r', 'زو': 'zou', 'ز': 'z', 'سو': 'sou', 'س': 's',\n",
        "        'شو': 'chou', 'ش': 'ch', 'صو': 'sou', 'ص': 's', 'ضو': 'dou', 'ض': 'd', 'طو': 'tou', 'ط': 't', 'ظو': 'dou',\n",
        "        'ظ': 'd', 'غو': 'ghou', 'غ': 'gh', 'فو': 'fou', 'ف': 'f',\n",
        "        'قو': 'qou', 'ق': 'q', 'كو': 'kou', 'ك': 'k', 'لو': 'lou', 'ل': 'l', 'مو': 'mou', 'م': 'm', 'نو': 'nou',\n",
        "        'ن': 'n', 'هو': 'hou', 'ه': 'h', 'وو': 'wou', 'و': 'w',\n",
        "        'يو': 'you', 'يي': 'yi', 'ي': 'i', 'ع': 'a', 'ة': 'a', 'ھ': 'h', 'ی': 'a', ' ':' '\n",
        "    }\n",
        "    return arabic, roman, to_latin\n",
        "\n",
        "def translit_arabic(text, arabic, roman, to_latin):\n",
        "    text = text.split(' ')\n",
        "    for part in text:\n",
        "        # Use dictionary\n",
        "        if part in arabic:\n",
        "            new = roman[arabic == part][0]\n",
        "        # Use transliterator\n",
        "        else:\n",
        "            for k, v in to_latin.items():\n",
        "                new = re.sub(k, v, part)\n",
        "        text =list(map(lambda x: x.replace(part, new), text))\n",
        "    text = ' '.join(text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQyZbEPMpwvU"
      },
      "source": [
        "****"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw6xOnizFZlS"
      },
      "source": [
        "**Arabic Name Disambiguation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "kC2ppLLj0B_L"
      },
      "outputs": [],
      "source": [
        "def main(list_n):  \n",
        "    # Normalization\n",
        "    list_n = normalize(list_n)\n",
        "    list_n = [sort(name) for name in list_n]\n",
        "    # Transliteration\n",
        "    arabic, roman, to_latin = load_() \n",
        "    list_n = [translit_arabic(name, arabic, roman, to_latin) for name in list_n]\n",
        "    # Fuzzy matching\n",
        "    distances = cdist(list_n, list_n, scorer=jarowinkler_similarity, score_cutoff=0.85, dtype=numpy.uint8, workers=-1)\n",
        "    indx_l = numpy.stack(numpy.where(distances >= 0.85), axis = 1)\n",
        "    rates = numpy.array([jellyfish.jaro_distance(jellyfish.match_rating_codex(list_n[indx[0]]),\n",
        "                            jellyfish.match_rating_codex(list_n[indx[1]])) for indx in indx_l])\n",
        "    indx_l = indx_l[numpy.where(rates >= 0.85)]\n",
        "    # Find connected components \n",
        "    g = nx.Graph()\n",
        "    for p in indx_l:\n",
        "        g.add_edges_from(zip(p, p[1:]))\n",
        "    g = nx.connected_components(g)\n",
        "    g = list(g)\n",
        "    #  \n",
        "    return g"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
